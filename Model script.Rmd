---
title: "Humana Competition Modeling Scripts"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(data.table)
library(readxl)
library(tidyverse)
library(VIM)
library(dplyr)
library(recipes)
library(caret)
```

# Data pre-processing

## data cleaning
add Xingran's part here

* '../training_new.csv' is the cleaned version of dataset

## categorical data processing

```{r}
######### data input
new = fread('../training_new.csv') %>% 
  janitor::clean_names() %>% 
  as.data.frame()
# new_cat = new[,..col_cat]
new = new[,-c(1,2)]

heading = read_excel('../Humana_Mays_2022_DataDictionary.xlsx', sheet = 'Data Dictionary')
heading = heading %>% janitor::clean_names()

######### change categorical data type to factor
## idnex for categorical variable (string + some integer index)
col_cat = heading[which(heading$data_type=='string'),]$feature_name
int_idx = c("cms_disabled_ind", "cons_hxmioc", "cons_hxmboh", "cons_stlnindx", "cmsd2_men_mad_ind", "cms_institutional_ind", "cms_hospice_ind", "cms_dual_eligible_ind", "cons_stlindex", "cms_low_income_ind", "cms_ma_plan_ind", "cons_hxmh", "cms_frailty_ind")
cat_idx = c(col_cat, int_idx)

new = new %>% 
  mutate_at(cat_idx, as.factor)

# num_idx = names(new)[sapply(new,is.numeric)]

## summary
# summary(new,maxsum = 15)
```

`new` is the cleaned dataset

## numeric data selection
Jing's part

```{r}
data_num <- new[ ,unlist(lapply(new, is.numeric))]
column_limit_na = names(which(colSums(is.na(data_num))<10000))
data_have_limit_na = data_num[,column_limit_na]

data_num2 = data_have_limit_na %>% 
  select(hi_flag,everything())
```

```{r}
rec1 = recipe(hi_flag ~ ., data = data_num2) %>%
  # step_impute_knn(all_predictors()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  step_nzv(all_predictors()) 
#%>%
#  step_pca(all_predictors(), threshold = .95) 

prep(rec1, training = data_num2, retain = TRUE) %>% 
  juice(all_predictors()) %>% 
  ncol()

select_features = prep(rec1, training = data_num2, retain = TRUE) %>% 
  juice(all_outcomes(),all_predictors())

process_select_vec = sort(colnames(select_features))

```

* `select_features` is the dataset of selected numeric features

## data imputation

```{r}
cat = new[,cat_idx]
all_selected = cbind(select_features,cat) %>% 
  mutate(hi_flag = as.factor(hi_flag))

### 1. Hot deck pmm imputation
library(mice)

start = Sys.time()
pmm = mice(all_selected, m = 5, method = "pmm")
new_all = complete(pmm,"repeated",include = TRUE)
### get the mode of repeated imputations

new_imp = data.frame(matrix(ncol = 0, nrow = nrow(new_all)))
for ( col in colnames(all_selected) ){
  subset = new_all %>% select(starts_with(paste0(col,'.')))
  new_imp[,col] = apply(subset, 1, function(x){
    if ( is.na(x[1]) ){
      if ( is.character(x[2]) | is.logical(x[2]))
        return( names(which.max(table(x[-1]))) ) ## the mode result in repeated pmm
      else if (is.integer(x[2]))
        return( median(x[-1]))
      else if (is.numeric(x[2]))
        return(mean(x[-1]))
      else {
        print(paste0('check the column ',col, ' datatype: ', typeof(x[2])) )
        return(x[1])
      }
    } else return(x[1])
  })
}
new_imp = new_imp %>%
  mutate_at(cat_idx,as.factor)

Sys.time() - start

# save(new_imp,file = "full_imputed.Rdata")
# load("full_imputed.Rdata")
```

* `new_imp` is the imputed dataset of all variables selected, variable saved in "full_imputed.Rdata"

## Feature selection
Jing's second part here

```{r}
new_imp$hi_flag = as.factor(new_imp$hi_flag)

# check na
# a = new_imp%>% 
#   summarise_all(funs(sum(is.na(.))))

# impute by median
new_imp <- new_imp %>% mutate(across(cnt_cp_vat_1, ~replace_na(., median(., na.rm=TRUE))))
```


```{r}
rfRFE <-  list(summary = defaultSummary,
               fit = function(x, y, first, last, ...){
                 library(randomForest)
                 randomForest(x, y, importance = first, ...)
                 },
               pred = function(object, x)  predict(object, x),
               rank = function(object, x, y) {
                 vimp <- varImp(object)
                 vimp <- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
                 vimp$var <- rownames(vimp)                  
                 vimp
                 },
               selectSize = pickSizeBest,
               selectVar = pickVars)


ctrl <- rfeControl(functions = rfRFE, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 1, # number of repeats
                      number = 10,
                   returnResamp = "all") # number of folds

library(doMC)
registerDoMC(cores = 2)
```

```{r}
start = Sys.time()

set.seed(10)
result_rfe1 <- rfe(x=new_imp[,2:237],y = new_imp$hi_flag, sizes = seq(100,230,10), rfeControl = ctrl)


# Print the results
result_rfe1

Sys.time() - start

```

```{r}
library(doMC)
registerDoMC(cores = 2)

set.seed(10)
result_rfe1 <- rfe(x=select_features[,3:199],y = select_features$id, sizes = seq(100,150,50), rfeControl = ctrl)


# Print the results
result_rfe1

# Print the selected features
predictors(result_rfe1)



trellis.par.set(caretTheme())
plot(result_rfe1, type = c("g", "o"))

# Print the results visually

```

## Feature Engineering

```{r}
### create metro column, 1 = metro counties, 0 = non-metro counties
new_imp_metro = new_imp %>% 
  mutate(metro = ifelse(grepl("Metro", rucc_category) , 1, 0),
         metro = as.factor(metro) )
```

* cat_metro - cat22 dataset with metro column



# Fairness problem (dataset manipulation)
Jing and Xingran

# Model Selection
Tips: scale and regularization before training
Yiru, Jiaqi

## logistic regression

## random forest

## xgboost
set.seed()

## lightGBM

## other models ...


# parameter tuning for models


# Model Evaluation
ddl: 10.1

