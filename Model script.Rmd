---
title: "Humana Competition Modeling Scripts"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE}
library(data.table)
library(readxl)
library(tidyverse)
library(VIM)
library(dplyr)
library(recipes)
library(caret)
library(ranger)
library(pROC)
<<<<<<< HEAD
library(xgboost)
library(keras)
library(tfruns)
=======
>>>>>>> 3b7a729a24b3e71e771fdbe35336cd2d9900e3c3
```

# Data pre-processing

## data cleaning
add Xingran's part here

* '../training_new.csv' is the cleaned version of dataset

## categorical data processing

```{r}
######### data input
new = fread('../training_new.csv') %>% 
  janitor::clean_names() %>% 
  as.data.frame()
# new_cat = new[,..col_cat]
new = new[,-c(1,2)]

heading = read_excel('../Humana_Mays_2022_DataDictionary.xlsx', sheet = 'Data Dictionary')
heading = heading %>% janitor::clean_names()

######### change categorical data type to factor
## idnex for categorical variable (string + some integer index)
<<<<<<< HEAD
col_cat = heading[which(heading$data_type=='string'),]$feature_name
int_idx = c("cms_disabled_ind", "cons_hxmioc", "cons_hxmboh", "cons_stlnindx", "cmsd2_men_mad_ind", "cms_dual_eligible_ind", "cons_stlindex", "cms_low_income_ind", "cons_hxmh", "cms_frailty_ind")
=======

col_cat = heading[which(heading$data_type=='string'),]$feature_name
int_idx = c("cms_disabled_ind", "cons_hxmioc", "cons_hxmboh", "cons_stlnindx", "cmsd2_men_mad_ind", "cms_dual_eligible_ind", "cons_stlindex", "cms_low_income_ind", "cons_hxmh", "cms_frailty_ind")

col_cat = heading[which(heading$data_type == 'string'),]$feature_name
int_idx = c("cms_disabled_ind", "cons_hxmioc", "cons_hxmboh", "cons_stlnindx", "cmsd2_men_mad_ind", "cms_institutional_ind", "cms_hospice_ind", "cms_dual_eligible_ind", "cons_stlindex", "cms_low_income_ind", "cms_ma_plan_ind", "cons_hxmh", "cms_frailty_ind")

>>>>>>> 3b7a729a24b3e71e771fdbe35336cd2d9900e3c3
cat_idx = c(col_cat, int_idx)

new = new %>% 
  mutate_at(cat_idx, as.factor) %>% 
  dplyr::select(- all_of(c("cms_institutional_ind", "cms_hospice_ind", "cms_ma_plan_ind"))) #cat variables with only one level

# num_idx = names(new)[sapply(new,is.numeric)]

## summary
# summary(new,maxsum = 15)
```

`new` is the cleaned dataset

## numeric data selection
Jing's part

```{r}
data_num <- new[ ,unlist(lapply(new, is.numeric))]
column_limit_na = names(which(colSums(is.na(data_num)) < 10000))
data_have_limit_na = data_num[,column_limit_na]

data_num2 = data_have_limit_na %>% 
  select(hi_flag,everything())
```

```{r}
rec1 = recipe(hi_flag ~ ., data = data_num2) %>%
  # step_impute_knn(all_predictors()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  step_nzv(all_predictors()) 
#%>%
#  step_pca(all_predictors(), threshold = .95) 

prep(rec1, training = data_num2, retain = TRUE) %>% 
  juice(all_predictors()) %>% 
  ncol()

select_features = prep(rec1, training = data_num2, retain = TRUE) %>% 
  juice(all_outcomes(),all_predictors())

process_select_vec = sort(colnames(select_features))

```

* `select_features` is the dataset of selected numeric features

## data imputation

```{r}
cat = new[,cat_idx]
all_selected = cbind(select_features,cat) %>% 
  mutate(hi_flag = as.factor(hi_flag))

### 1. Hot deck pmm imputation
library(mice)

start = Sys.time()
pmm = mice(all_selected, m = 5, method = "pmm")
new_all = complete(pmm,"repeated",include = TRUE)
### get the mode of repeated imputations

new_imp = data.frame(matrix(ncol = 0, nrow = nrow(new_all)))
for ( col in colnames(all_selected) ){
  subset = new_all %>% select(starts_with(paste0(col,'.')))
  new_imp[,col] = apply(subset, 1, function(x){
    if ( is.na(x[1]) ){
      if ( is.character(x[2]) | is.logical(x[2]))
        return( names(which.max(table(x[-1]))) ) ## the mode result in repeated pmm
      else if (is.integer(x[2]))
        return( median(x[-1]))
      else if (is.numeric(x[2]))
        return(mean(x[-1]))
      else {
        print(paste0('check the column ',col, ' datatype: ', typeof(x[2])) )
        return(x[1])
      }
    } else return(x[1])
  })
}
new_imp = new_imp %>%
  mutate_at(cat_idx,as.factor)

Sys.time() - start

save(new_imp,file = "full_imputed.Rdata")
# load("full_imputed.Rdata")
```

* `new_imp` is the imputed dataset of all variables selected, variable saved in "full_imputed.Rdata"

## Feature selection
Jing's second part here


## Feature Engineering

```{r}
load("full_imputed.Rdata")
### create metro column, 1 = metro counties, 0 = non-metro counties
new_imp_metro = new_imp %>% 
  mutate(metro = ifelse(grepl("Metro", rucc_category) , 1, 0),
         metro = as.factor(metro) )
```

* cat_metro - cat22 dataset with metro column

 

# Fairness problem (dataset manipulation)
Jing and Xingran

# Model Selection
Tips: scale and regularization before training

```{r}
final = new_imp_metro %>% 
  dplyr::select(- all_of(c("cms_institutional_ind", "cms_hospice_ind", "cms_ma_plan_ind")))
summary(final[,cat_idx],maxsum=10)

set.seed(1)
rowTrain <- createDataPartition(y = final$hi_flag,
                                p = 0.7,
                                list = FALSE)
x = final[rowTrain,-1]  ## training data
y = final$hi_flag[rowTrain]   
x2 = final[-rowTrain,-1]   ## testing data
y2 = final$hi_flag[-rowTrain]

save(x,y,x2,y2,rowTrain, final, file = "split_data.Rdata")
<<<<<<< HEAD
=======
save(x,y,x2,y2,file = "split_data.Rdata")
>>>>>>> 3b7a729a24b3e71e771fdbe35336cd2d9900e3c3

# load("split_data.Rdata")
```

# Imbalanced Data Processing with SMOTE
```{r}
table(final$hi_flag)

predictor_variables <- final[-1]
response_variable <- final$hi_flag

# running smote function on data
Smote_data <- ubBalance(predictor_variables, 
                        response_variable, 
                        type = 'ubSMOTE',     
                        k = 3,              # How many neighbouring data points to consider
                        percOver = 2180.45, # Percentage of minority cases 46182/2118 *100
                        percUnder = 100,    # Percentage of majority cases default 100
                        verbose = TRUE)
```


## Logistic Regression (Baseline)

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Fit logistic regression model
lm.fit <- lm(hi_flag ~ ., 
         data = final,
         subset = rowTrain,
         metric = "ROC")

# Evaluate lm performance on test data
test.pred.prob <- predict(lm.fit, 
                         newdata = final[-rowTrain,],
                         type = "response")

test.pred <- rep("0", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] <- "1"

confusionMatrix(data = factor(test.pred),
                reference = as.factor(final$hi_flag)[-rowTrain],
                positive = "1")

roc_lm <- roc(y2, test.pred.prob) ## 0.7262
```


## Random Forest

```{r}
final_rf = final %>% 
mutate(# rucc_category = gsub("-","_",rucc_category),
         # cms_ra_factor_type_cd = recode(cms_ra_factor_type_cd,`*`="unknown"),
         # lang_spoken_cd = recode(lang_spoken_cd,`*`="unknown"),
         hi_flag = as.factor(hi_flag) ) %>% 
  mutate_if(is.factor,function(x) factor(x, labels = make.names(levels(x))))

rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10,
                                           by = 2))

rf.fit <- train(hi_flag ~ .,
                data = final_rf,
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)
```

```{r}
# variable importance
rf2.final.per <- ranger(hi_flag ~ . ,
                final_rf[rowTrain,],
                mtry = rf.fit$bestTune[[1]],
                min.node.size = rf.fit$bestTune[[3]],
                splitrule = "gini",
                importance = "permutation",
                scale.permutation.importance = TRUE)

par(mar = c(3,12,3,3))
barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE)[1:20],
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("cyan","blue"))(10))
```

```{r}
## test
# rf.pred <- predict(rf.fit, newdata = data[-rowTrain,], type = "raw")
rf.pred <- predict(rf.fit, newdata = final_rf[-rowTrain,], type = "prob")[,2]
roc.rf <- roc(y2, rf.pred)
```

roc = 0.7061

## LightGBM

```{r}
library(lightgbm)
library(MLmetrics)

final_lgb = final %>% 
  mutate(hi_flag = as.numeric(hi_flag)) %>% 
  mutate_if(is.factor, as.numeric) %>% 
  as.matrix()
  
lgb_train = lgb.Dataset(data=final_lgb[rowTrain,-1],
                        label=final_lgb[rowTrain,1],
                        categorical_feature = cat_idx)

x2_lgb = final_lgb[-rowTrain,-1]
y2_lgb = final_lgb[-rowTrain,1]
lgb_test = lgb.Dataset(data=x2_lgb,
                       label=y2_lgb,
                       categorical_feature = cat_idx)
```

```{r}
lgb.grid = list(objective = "binary",
                metric = "auc",
                min_sum_hessian_in_leaf = 1,
                feature_fraction = 0.7,
                bagging_fraction = 0.7,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 8,
                lambda_l2 = 1.3,
                min_data_in_bin=100,
                min_gain_to_split = 10,
                min_data_in_leaf = 30,
                is_unbalance = TRUE)

# Evaluation: Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
  actual = getinfo(lgb_train, "label")
  score  = NormalizedGini(preds,actual)
  return(list(name = "gini", value = score, higher_better = TRUE))
}
```

```{r}
start = Sys.time()
start

lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb_train, learning_rate = 0.01, num_leaves = 25,is_unbalance = TRUE,
                  num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
                  eval_freq = 20, eval = lgb.normalizedgini,
                  categorical_feature = cat_idx, nfold = 5, stratified = TRUE)

best.iter = lgb.model.cv$best_iter
lgb.model.cv$best_score

Sys.time() - start
```

### Grid Tuning

```{r}
grids = expand.grid(# max_depth = seq(3,15,3),
                # learning_rate = c(0.01,0.005,0.001),
                # min_sum_hessian_in_leaf = seq(0.0005,0.01,0.002),
                feature_fraction = c(0.3,0.5),
                # lambda_l1 = c(1,2,3),
                lambda_l2 = c(1:3,1.3, 0.5)
                # min_gain_to_split = seq(1,15,4),
                # min_data_in_leaf = c(30,100,500,1000,2000)
                # bagging_fraction = 0.7,
                # bagging_freq = 5,
                # min_data = 100,
                # max_bin = 50,
                # min_data_in_bin=100
                )

start = Sys.time()
start

performance = data.frame(matrix(ncol = 3, nrow = 0))
colnames(performance) = c('comb','best_iter', "best_score")
for (i in c(1:nrow(grids))){
  lgb.grid = list(objective = "binary",
                metric = "auc",
                # max_depth = grids$max_depth[i],
                learning_rate = 0.01,
                min_sum_hessian_in_leaf = 1,
                feature_fraction = grids$feature_fraction[i],
                bagging_fraction = 0.7,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 3,
                lambda_l2 = grids$lambda_l2[i],
                min_data_in_bin=100,
                # min_gain_to_split = grids$min_gain_to_split[i],
                min_data_in_leaf = 30,
                # is_unbalance = TRUE,
                scale_pos_weight = 24)
  
  lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb_train, num_leaves = 25,
                  num_threads = 4 , nrounds = 7000, early_stopping_rounds = 50,
                  eval_freq = 20, eval = lgb.normalizedgini,
                  categorical_feature = cat_idx, nfold = 5, stratified = TRUE)
  record = data.frame(comb=i, 
                      best_iter=lgb.model.cv$best_iter, 
                      best_score=lgb.model.cv$best_score)
  print(record)
  performance = rbind(performance,record)
}

best = performance[which.max(performance$best_score),]
print(best)

p3 = cbind(performance, grids) %>%
  arrange(desc(best_score))


Sys.time() - start
```

comb best_iter best_score learning_rate feature_fraction lambda_l1 lambda_l2
  226       359  0.7515991          0.01              0.3         3       1.3

```{r}
# Train final model
i = best$comb
lgb.grid = list(objective = "binary",
                metric = "auc",
                # max_depth = grids$max_depth[i],
                learning_rate = p2$learning_rate[1],
                min_sum_hessian_in_leaf = 1,
                feature_fraction = p2$feature_fraction[1],
                bagging_fraction = 0.7,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = p2$lambda_l1[1],
                lambda_l2 = p2$lambda_l2[1],
                min_data_in_bin=100,
                # min_gain_to_split = grids$min_gain_to_split[i],
                min_data_in_leaf = 30,
                is_unbalance = TRUE)
                # scale_pos_weight = 24)

lgb.model = lgb.train(params = lgb.grid, data = lgb_train, learning_rate = 0.01, #num_leaves = 25,
                      num_threads = 2 , nrounds = best$best_iter,
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = cat_idx)

lgb_pred = predict(lgb.model,x2_lgb)
roc(y2_lgb, lgb_pred)
```

ROC = 0.7296

## XGBoost

```{r}
# Convert final data set to matrix
final_matrix = model.matrix(hi_flag ~., final)[ , -1]

# get the numb 70/30 training test split
train_samples <- round(length(final$hi_flag) * .7)

# training data
train_data <- final_matrix[1:train_samples,]
train_labels <- final$hi_flag[1:train_samples]

# testing data
test_data <- final_matrix[-(1:train_samples),]
test_labels <- final$hi_flag[-(1:train_samples)]

# Load XGBoost package
library(xgboost)

# Convert the cleaned dataframe to a matrix
dtrain <- xgb.DMatrix(data = train_data, label = train_labels)
dtest <- xgb.DMatrix(data = test_data, label = test_labels)
```

### Model Training
```{r}
# train a model using our training data
xgboost_model <- xgboost(data = dtrain,   
                 nround = 2, 
                 objective = "binary:logistic")  

### [1]	train-logloss:0.478371 
### [2]	train-logloss:0.360748 

# generate predictions for our held-out testing data
pred <- predict(xgboost_model, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.5) != test_labels)
print(paste("test-error=", err))

### test-error= 0.0420289855072464

# ROC
roc_xgboost <- roc(test_labels, pred) ## 0.7031
```

### Model Turning 
```{r}
# train an xgboost model
model_tuned <- xgboost(data = dtrain,       
                 max.depth = 3, 
                 nround = 2, 
                 objective = "binary:logistic") 

### [1]	train-logloss:0.479854 
### [2]	train-logloss:0.363968 

# generate predictions for our held-out testing data
pred2 <- predict(model_tuned, dtest)

# get & print the classification error
err2 <- mean(as.numeric(pred2 > 0.5) != test_labels)
print(paste("test-error=", err2))

### test-error= 0.0420289855072464

# ROC
roc_xgboost2 <- roc(test_labels, pred2) ## 0.6938
```

### Paramer Tuning
```{r}
# get the number of negative & positive cases in our data
negative_cases <- sum(train_labels == 0) ## 0 = FALSE
postive_cases <- sum(train_labels == 1) ##1 = TRUE

# train a model using our training data
model_tuned2 <- xgboost(data = dtrain,           
                 max.depth = 3, 
                 nround = 10, 
                 early_stopping_rounds = 3, 
                 objective = "binary:logistic",
                 scale_pos_weight = negative_cases/postive_cases,
                 gamma = 1) 

# generate predictions for our held-out testing data
pred3 <- predict(model_tuned2, dtest)

# get & print the classification error
err3 <- mean(as.numeric(pred3 > 0.5) != test_labels)
print(paste("test-error=", err3))

### test-error = 0.296618357487923

# ROC
roc_xgboost3 <- roc(test_labels, pred3) ## 0.7268
```

### Model Examining
```{r}
library(base)
library(DiagrammeR)
# Plot fratures
xgb.plot.multi.trees(feature_names = names(final_matrix),
                     model = xgboost_model)

# Get information on how important each feature is
importance_matrix <- xgb.importance(names(final_matrix), model = xgboost_model)

# Important plot 
xgb.plot.importance(importance_matrix)
```


## Neural Networks

```{r}
# load("split_data.Rdata")
## tuning
runs <- tuning_run("keras_grid_search.R", 
                   flags = list(
                   nodes_layer1 = c(64, 128, 256),
                   nodes_layer2 = c(64, 128, 256),
                   nodes_layer3 = c(64, 128, 256),
                   dropout_layer1 = c(0.2, 0.3, 0.4),
                   dropout_layer2 = c(0.2, 0.3, 0.4),
                   dropout_layer3 = c(0.2, 0.3, 0.4)),
                   confirm = FALSE,
                   echo = FALSE,
                   sample = 0.01) # try more after class

best = runs[which.max(runs$metric_val_accuracy),]
best
```

```{r}
model.nn <- keras_model_sequential() %>%
  layer_dense(units = best$flag_nodes_layer1, activation = "relu", input_shape = ncol(x)) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = best$flag_dropout_layer1) %>%
  layer_dense(units = best$flag_nodes_layer2, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = best$flag_dropout_layer2) %>%
  layer_dense(units = best$flag_nodes_layer3, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = best$flag_dropout_layer3) %>%
  layer_dense(units = 2, activation = "sigmoid") %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_rmsprop(), 
          metrics = "roc") 
fit.nn = model.nn %>% 
  fit(x = x, 
      y = y, 
      epochs = 50, 
      batch_size = 256,
      validation_split = 0.2,
      callbacks = list(callback_early_stopping(patience = 20),
                       callback_reduce_lr_on_plateau()),
      verbose = 2)
plot(fit.nn)

## testing and evaluation
score <- model.nn %>% evaluate(x2, y2_c)
score
```


# Model Evaluation
ddl: 10.1

