---
title: "Humana Competition Modeling Scripts"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE}
library(data.table)
library(readxl)
library(tidyverse)
library(VIM)
library(dplyr)
library(recipes)
library(caret)
library(ranger)
library(pROC)
```

# Data pre-processing

## data cleaning
add Xingran's part here

* '../training_new.csv' is the cleaned version of dataset

## categorical data processing

```{r}
######### data input
new = fread('../training_new.csv') %>% 
  janitor::clean_names() %>% 
  as.data.frame()
# new_cat = new[,..col_cat]
new = new[,-c(1,2)]

heading = read_excel('../Humana_Mays_2022_DataDictionary.xlsx', sheet = 'Data Dictionary')
heading = heading %>% janitor::clean_names()

######### change categorical data type to factor
## idnex for categorical variable (string + some integer index)

col_cat = heading[which(heading$data_type=='string'),]$feature_name
int_idx = c("cms_disabled_ind", "cons_hxmioc", "cons_hxmboh", "cons_stlnindx", "cmsd2_men_mad_ind", "cms_dual_eligible_ind", "cons_stlindex", "cms_low_income_ind", "cons_hxmh", "cms_frailty_ind")

col_cat = heading[which(heading$data_type == 'string'),]$feature_name
int_idx = c("cms_disabled_ind", "cons_hxmioc", "cons_hxmboh", "cons_stlnindx", "cmsd2_men_mad_ind", "cms_institutional_ind", "cms_hospice_ind", "cms_dual_eligible_ind", "cons_stlindex", "cms_low_income_ind", "cms_ma_plan_ind", "cons_hxmh", "cms_frailty_ind")

cat_idx = c(col_cat, int_idx)

new = new %>% 
  mutate_at(cat_idx, as.factor) %>% 
  dplyr::select(- all_of(c("cms_institutional_ind", "cms_hospice_ind", "cms_ma_plan_ind"))) #cat variables with only one level

# num_idx = names(new)[sapply(new,is.numeric)]

## summary
# summary(new,maxsum = 15)
```

`new` is the cleaned dataset

## numeric data selection
Jing's part

```{r}
data_num <- new[ ,unlist(lapply(new, is.numeric))]
column_limit_na = names(which(colSums(is.na(data_num)) < 10000))
data_have_limit_na = data_num[,column_limit_na]

data_num2 = data_have_limit_na %>% 
  select(hi_flag,everything())
```

```{r}
rec1 = recipe(hi_flag ~ ., data = data_num2) %>%
  # step_impute_knn(all_predictors()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  step_nzv(all_predictors()) 
#%>%
#  step_pca(all_predictors(), threshold = .95) 

prep(rec1, training = data_num2, retain = TRUE) %>% 
  juice(all_predictors()) %>% 
  ncol()

select_features = prep(rec1, training = data_num2, retain = TRUE) %>% 
  juice(all_outcomes(),all_predictors())

process_select_vec = sort(colnames(select_features))

```

* `select_features` is the dataset of selected numeric features

## data imputation

```{r}
cat = new[,cat_idx]
all_selected = cbind(select_features,cat) %>% 
  mutate(hi_flag = as.factor(hi_flag))

### 1. Hot deck pmm imputation
library(mice)

start = Sys.time()
pmm = mice(all_selected, m = 5, method = "pmm")
new_all = complete(pmm,"repeated",include = TRUE)
### get the mode of repeated imputations

new_imp = data.frame(matrix(ncol = 0, nrow = nrow(new_all)))
for ( col in colnames(all_selected) ){
  subset = new_all %>% select(starts_with(paste0(col,'.')))
  new_imp[,col] = apply(subset, 1, function(x){
    if ( is.na(x[1]) ){
      if ( is.character(x[2]) | is.logical(x[2]))
        return( names(which.max(table(x[-1]))) ) ## the mode result in repeated pmm
      else if (is.integer(x[2]))
        return( median(x[-1]))
      else if (is.numeric(x[2]))
        return(mean(x[-1]))
      else {
        print(paste0('check the column ',col, ' datatype: ', typeof(x[2])) )
        return(x[1])
      }
    } else return(x[1])
  })
}
new_imp = new_imp %>%
  mutate_at(cat_idx,as.factor)

Sys.time() - start

save(new_imp,file = "full_imputed.Rdata")
# load("full_imputed.Rdata")
```

* `new_imp` is the imputed dataset of all variables selected, variable saved in "full_imputed.Rdata"

## Feature selection
Jing's second part here

```{r}
new_imp$hi_flag = as.factor(new_imp$hi_flag)
levels(new_imp$hi_flag) = c("no","yes")
# check na
# a = new_imp%>% 
#   summarise_all(funs(sum(is.na(.))))

# impute by median
new_imp <- new_imp %>% mutate(across(cnt_cp_vat_1, ~replace_na(., median(., na.rm=TRUE))))
```


```{r}
rfRFE <-  list(summary = defaultSummary,
               fit = function(x, y, first, last, ...){
                 library(randomForest)
                 randomForest(x, y, importance = first, ...)
                 },
               pred = function(object, x)  predict(object, x),
               rank = function(object, x, y) {
                 vimp <- varImp(object)
                 vimp <- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
                 vimp$var <- rownames(vimp)                  
                 vimp
                 },
               selectSize = pickSizeBest,
               selectVar = pickVars)


ctrl <- rfeControl(functions = rfRFE, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 1, # number of repeats
                      number = 10,
                   returnResamp = "all") # number of folds

library(doMC)
registerDoMC(cores = 2)
```

```{r}
start = Sys.time()

set.seed(10)
result_rfe1 <- rfe(x=new_imp[,2:237],y = new_imp$hi_flag, sizes = seq(100,230,10), rfeControl = ctrl)


# Print the results
result_rfe1

Sys.time() - start

```

```{r}
library(doMC)
registerDoMC(cores = 2)

set.seed(10)
result_rfe1 <- rfe(x=select_features[,3:199],y = select_features$id, sizes = seq(100,150,50), rfeControl = ctrl)


# Print the results
result_rfe1

# Print the selected features
predictors(result_rfe1)



trellis.par.set(caretTheme())
plot(result_rfe1, type = c("g", "o"))

# Print the results visually

```

## Feature Engineering

```{r}
load("full_imputed.Rdata")
### create metro column, 1 = metro counties, 0 = non-metro counties
new_imp_metro = new_imp %>% 
  mutate(metro = ifelse(grepl("Metro", rucc_category) , 1, 0),
         metro = as.factor(metro) )
```

* cat_metro - cat22 dataset with metro column

 

# Fairness problem (dataset manipulation)
Jing and Xingran

# Model Selection
Tips: scale and regularization before training

```{r}
final = new_imp_metro %>% 
  dplyr::select(- all_of(c("cms_institutional_ind", "cms_hospice_ind", "cms_ma_plan_ind")))
summary(final[,cat_idx],maxsum=10)

set.seed(1)
rowTrain <- createDataPartition(y = final$hi_flag,
                                p = 0.7,
                                list = FALSE)
x = final[rowTrain,-1]  ## training data
y = final$hi_flag[rowTrain]   
x2 = final[-rowTrain,-1]   ## testing data
y2 = final$hi_flag[-rowTrain]

save(x,y,x2,y2,rowTrain, final, file = "split_data.Rdata")
save(x,y,x2,y2,file = "split_data.Rdata")

# load("split_data.Rdata")
```

# Imbalanced Data Processing

### Analysis of Original Dataset
```{r}
table(final$hi_flag) # 0: 46182, 1: 2118
prop.table(table(final$hi_flag))  # 0: 0.956, 1: 0.04

predictor_variables <- final[-1]
response_variable <- final$hi_flag

library(ROSE)
library(rpart)

# Original prediction accuracy using decision tree algorithm
treeimb <- rpart(hi_flag ~ ., data = cbind(x, hi_flag = y))
pred.treeimb <- predict(treeimb, newdata = cbind(x2, hi_flag = y2))

accuracy.meas(response = y2, predicted = pred.treeimb[,1])
# F = 0.042 is low and suggests weak accuracy of this model

roc.curve(y2, pred.treeimb[,2], plotit = F) 
# AUC = 0.5 is a low score, suggesting the poor performance of the model with original data set.
```

### Data Balancing Models
```{r}
# Over sampling
data_balanced_over <- ovun.sample(hi_flag ~ ., 
                                  data = cbind(x, hi_flag = y),
                                  method = "over",
                                  p = 0.5)$data

table(data_balanced_over$hi_flag)  # 0: 32328 1: 32511

# Under sampling
data_balanced_under <- ovun.sample(hi_flag ~ ., 
                                  data = cbind(x, hi_flag = y),
                                  method = "under",
                                  p = 0.5)$data

table(data_balanced_under$hi_flag) # 0: 1448 1: 1483

# Both over and under sampling
data_balanced_both <- ovun.sample(hi_flag ~ ., 
                                  data = cbind(x, hi_flag = y),
                                  method = "both",
                                  p = 0.5)$data

table(data_balanced_both$hi_flag) # 0: 16873 1: 16938

# Data balancing using ROSE
data.rose <- ROSE(hi_flag ~ ., 
                  data = cbind(x, hi_flag = y), 
                  seed = 1)$data

table(data.rose$hi_flag)   # 0: 17000 1: 16811
```

### Compute Models using Each Data and Evaluate its Accuracy
```{r}
#build decision tree models
tree.rose <- rpart(hi_flag ~ ., data = data.rose)
tree.over <- rpart(hi_flag ~ ., data = data_balanced_over)
tree.under <- rpart(hi_flag ~ ., data = data_balanced_under)
tree.both <- rpart(hi_flag ~ ., data = data_balanced_both)

#make predictions on unseen data
pred.tree.rose <- predict(tree.rose, newdata = cbind(x2, hi_flag = y2))
pred.tree.over <- predict(tree.over, newdata = cbind(x2, hi_flag = y2))
pred.tree.under <- predict(tree.under, newdata = cbind(x2, hi_flag = y2))
pred.tree.both <- predict(tree.both, newdata = cbind(x2, hi_flag = y2))

# ROC ROSE
roc.curve(y2, pred.tree.rose[,2])
# ROC: 0.648

# ROC oversampling
roc.curve(y2, pred.tree.over[,2])
# ROC: 0.641

# ROC undersampling
roc.curve(y2, pred.tree.under[,2])
# ROC: 0.656

# ROC both
roc.curve(y2, pred.tree.both[,2])
# ROC: 0.643
```

We choose ROSE as our method of data balancing

###  
```{r}
save(data.rose, file = "balanced_data.Rdata")

# load("balanced_data.Rdata")
```


## Logistic Regression (Baseline)

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Fit logistic regression model
lm.fit <- lm(hi_flag ~ ., 
         data = final,
         subset = rowTrain,
         metric = "ROC")

# Evaluate lm performance on test data
test.pred.prob <- predict(lm.fit, 
                         newdata = final[-rowTrain,],
                         type = "response")

test.pred <- rep("0", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] <- "1"

confusionMatrix(data = factor(test.pred),
                reference = as.factor(final$hi_flag)[-rowTrain],
                positive = "1")

roc_lm <- roc(y2, test.pred.prob) ## 0.7262
```


## Random Forest

```{r}
final_rf = final %>% 
mutate(# rucc_category = gsub("-","_",rucc_category),
         # cms_ra_factor_type_cd = recode(cms_ra_factor_type_cd,`*`="unknown"),
         # lang_spoken_cd = recode(lang_spoken_cd,`*`="unknown"),
         hi_flag = as.factor(hi_flag) ) %>% 
  mutate_if(is.factor,function(x) factor(x, labels = make.names(levels(x))))

rf.grid <- expand.grid(mtry = 1:8,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10,
                                           by = 2))

rf.fit <- train(hi_flag ~ .,
                data = final_rf,
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)
```

```{r}
# variable importance
rf2.final.per <- ranger(hi_flag ~ . ,
                final_rf[rowTrain,],
                mtry = rf.fit$bestTune[[1]],
                min.node.size = rf.fit$bestTune[[3]],
                splitrule = "gini",
                importance = "permutation",
                scale.permutation.importance = TRUE)

par(mar = c(3,12,3,3))
barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE)[1:20],
        las = 2, horiz = TRUE, cex.names = 0.7, 
        col = colorRampPalette(colors = c("cyan","blue"))(10))
```

```{r}
## test
# rf.pred <- predict(rf.fit, newdata = data[-rowTrain,], type = "raw")
rf.pred <- predict(rf.fit, newdata = final_rf[-rowTrain,], type = "prob")[,2]
roc.rf <- roc(y2, rf.pred)
```

roc = 0.7061

## LightGBM

```{r}
library(Matrix)
library(lightgbm)
library(MLmetrics)

final_lgb = final %>% 
  mutate(hi_flag = as.numeric(hi_flag)) %>% 
  mutate_if(is.factor, as.numeric) %>% 
  as.matrix()
  
lgb_train = lgb.Dataset(data=final_lgb[rowTrain,-1],
                        label=final_lgb[rowTrain,1],
                        categorical_feature = cat_idx)

x2_lgb = final_lgb[-rowTrain,-1]
y2_lgb = final_lgb[-rowTrain,1]
lgb_test = lgb.Dataset(data=x2_lgb,
                       label=y2_lgb,
                       categorical_feature = cat_idx)
```

```{r}
lgb.grid = list(objective = "binary",
                metric = "auc",
                min_sum_hessian_in_leaf = 1,
                feature_fraction = 0.7,
                bagging_fraction = 0.7,
                bagging_freq = 5,
                min_data = 100,
                max_bin = 50,
                lambda_l1 = 8,
                lambda_l2 = 1.3,
                min_data_in_bin=100,
                min_gain_to_split = 10,
                min_data_in_leaf = 30,
                is_unbalance = TRUE)

# Evaluation: Gini for Lgb
lgb.normalizedgini = function(preds, dtrain){
  actual = getinfo(lgb_train, "label")
  score  = NormalizedGini(preds,actual)
  return(list(name = "gini", value = score, higher_better = TRUE))
}
```

```{r}
start = Sys.time()
start

lgb.model.cv = lgb.cv(params = lgb.grid, data = lgb_train, learning_rate = 0.02, num_leaves = 25,
                  num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
                  eval_freq = 20, eval = lgb.normalizedgini,
                  categorical_feature = cat_idx, nfold = 5, stratified = TRUE)

best.iter = lgb.model.cv$best_iter

Sys.time() - start
```

```{r}
# Train final model
lgb.model = lgb.train(params = lgb.grid, data = lgb_train, learning_rate = 0.02,
                      num_leaves = 25, num_threads = 2 , nrounds = best.iter,
                      eval_freq = 20, eval = lgb.normalizedgini,
                      categorical_feature = cat_idx)

lgb_pred = predict(lgb.model,x2_lgb)
roc.lgb <- roc(y2_lgb, lgb_pred)
```

ROC = 0.7184

## XGBoost

```{r}
# Convert final data set to matrix
final_matrix = model.matrix(hi_flag ~., final)[ , -1]

# get the numb 70/30 training test split
train_samples <- round(length(final$hi_flag) * .7)

# training data
train_data <- final_matrix[1:train_samples,]
train_labels <- final$hi_flag[1:train_samples]

# testing data
test_data <- final_matrix[-(1:train_samples),]
test_labels <- final$hi_flag[-(1:train_samples)]

# Load XGBoost package
library(xgboost)

# Convert the cleaned dataframe to a matrix
dtrain <- xgb.DMatrix(data = train_data, label = train_labels)
dtest <- xgb.DMatrix(data = test_data, label = test_labels)
```

### Model Training
```{r}
# train a model using our training data
xgboost_model <- xgboost(data = dtrain,   
                 nround = 2, 
                 objective = "binary:logistic")  

### [1]	train-logloss:0.478371 
### [2]	train-logloss:0.360748 

# generate predictions for our held-out testing data
pred <- predict(xgboost_model, dtest)

# get & print the classification error
err <- mean(as.numeric(pred > 0.5) != test_labels)
print(paste("test-error=", err))

### test-error= 0.0420289855072464

# ROC
roc_xgboost <- roc(test_labels, pred) ## 0.7031
```

### Model Turning 
```{r}
# train an xgboost model
model_tuned <- xgboost(data = dtrain,       
                 max.depth = 3, 
                 nround = 2, 
                 objective = "binary:logistic") 

### [1]	train-logloss:0.479854 
### [2]	train-logloss:0.363968 

# generate predictions for our held-out testing data
pred2 <- predict(model_tuned, dtest)

# get & print the classification error
err2 <- mean(as.numeric(pred2 > 0.5) != test_labels)
print(paste("test-error=", err2))

### test-error= 0.0420289855072464

# ROC
roc_xgboost2 <- roc(test_labels, pred2) ## 0.6938
```

### Paramer Tuning
```{r}
# get the number of negative & positive cases in our data
negative_cases <- sum(train_labels == 0) ## 0 = FALSE
postive_cases <- sum(train_labels == 1) ##1 = TRUE

# train a model using our training data
model_tuned2 <- xgboost(data = dtrain,           
                 max.depth = 3, 
                 nround = 10, 
                 early_stopping_rounds = 3, 
                 objective = "binary:logistic",
                 scale_pos_weight = negative_cases/postive_cases,
                 gamma = 1) 

# generate predictions for our held-out testing data
pred3 <- predict(model_tuned2, dtest)

# get & print the classification error
err3 <- mean(as.numeric(pred3 > 0.5) != test_labels)
print(paste("test-error=", err3))

### test-error = 0.296618357487923

# ROC
roc_xgboost3 <- roc(test_labels, pred3) ## 0.7268
```

### Model Examining
```{r}
library(base)
library(DiagrammeR)
# Plot fratures
xgb.plot.multi.trees(feature_names = names(final_matrix),
                     model = xgboost_model)

# Get information on how important each feature is
importance_matrix <- xgb.importance(names(final_matrix), model = xgboost_model)

# Important plot 
xgb.plot.importance(importance_matrix)
```


## other models ...


# parameter tuning for models


# Model Evaluation
ddl: 10.1

